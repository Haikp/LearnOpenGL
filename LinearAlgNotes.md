I had taken notes on 3b1b's series for linear algebra, which helped get a start on learning graphic programming. I just wanted to leave my notes here for keeps sake, and as well as a place I could return to if I needed a quick refresher.

## Chapter 1 - Vectors
Rather than repeating the same computation of vectors, its more important to understand more about the context of a vector. In the case of computer graphics, we able to use vectors such that we can describe a space, and as well as manipulate it in the way that we want.

## Chapter 2 - Linear Combination, Span, and Basis Vectors
Basis vectors are essentially a systems' "base" vectors that are scaled to obtain a vector. If the basis vectors are linearly independent from one another, we are able to see that any linear combination of scalars using the basis vectors can create any vector as long as the initial point is at the origin. The space in which these possible vectors exist is called the span.

As a visual concept, on a 2D plane given that both vectors are linearly independent, we understand that it encompasses the entire 2D grid. Referring to a 3D grid, given that all 3 vectors are linearly independent, the span of the basis vectors encompasses all of 3D space.

In the case of linearly dependent vectors, meaning that the vectors fall in the same plane as another vector, we are limited to one less dimension. If in 2D we had two linearly dependent vectors, we would effectively only be able to travel in a line. In the case of a 3D vector where 2 vectors are linearly dependent, and the last vector linearly independent, we would only be able to travel in a plane.

The basis, fundamental vectors, of a vector space is the set of linearly independent vectors that span the full space.

## Chapter 3 - Linear Transformations and Matrices
Transformations as a whole can be understood as applying changes to the basis vectors such that all resulting vectors follow suit. As a mathematical understanding, a vector is determined by multiplying the desired scalar by the basis vector correspondingly. Using "i hat" as the basis vector for the x axis, and "j hat" as the basis vector for the y axis, to obtain a resulting vector we insert our desired scalars such that the result is:

					[ x("i hat") y("j hat") ]

The result is then similar to a function where it outputs a vector in the perspective of that basis vector. In transforming the basis vectors, we manipulate the space of these basis vectors such that inserting the same scalar would result in a different outcome. A cool way to visually these transformations is through using the basis vectors, using them as a way to size up the dimensions' grid. The x-axis is spaced evenly by the length of "i hat", while the y-axis is spaced evenly by the length of "j hat".

If we were to place "i hat" and "j hat" in a matrix by columns, then we would have a way to mathematically compute any transformation in the perspective of "i hat" and "j hat" through matrix multiplication.

## Chapter 4 - Matrix Multiplication as Composition
A composition of a matrix can be understood as the combination of multiple transformations applied to a matrix through multiplication. It is important to note that the order of a transformation is important, it is not commutative, but it is associative. The practice which should be applied is reading transformations from right to left. An important concept to understand behind matrix multiplication is that it is essentially apply a transformation, then another transformation from right to left.

## Chapter 5 - 3D Linear Transformation
There are no special rules up to this point looking at 3D space. Everything in the 2D space that was learned can be applied seamlessly in to 3D space.

## Chapter 6 - The Determinant
The determinant is simply defined as the scaling factor in which an area receives once a transformation occurs. That's it. Note that there are negative values that can occur, to which this represents the orientation of the shape after transformation. Looking at the axes, if at any point the x and y axes cross the 0 or 180 degree point, the values of the determinant will be negative. The mental image associate with negative determinants is inverting or flipping the object inside out, or flipping or a piece of paper in a 2D plane.

## Chapter 7 - Inverse Matrices, Column Space, and Null Space
The concept of an inverse matrix is to be able to multiply it to a transformation such that we receive the identity matrix. Using this inverse matrix would allow us to view the original inputs to an already transformed vector. This is useful the the case of solving a for a systems of equations. In the case that the determinant of a matrix is 0, there is no inverse matrix. To note with 3D planes, a 3D object flattening to a 2D plane results in a determinant of 0. There is a possible solution, if and only if the solution is already on the same line of vectors as the linearly dependent span. 

In reference to linearly dependency, the number of dimensions in which a matrix can represent is referred to by its rank, with the number of dimensions being the quantitative value. A 2D matrix with linearly dependent vectors would have a rank of 1. In the case linear independency, it would have what's defined as the max rank. In the case of a 3D space, it can potentially have a rank of 1, 2, and max of 3. The set of all possible outputs of scalar and basis vectors refers to its column space. Span of columns = column space. 

The null space can be defined as the set of vectors that result in 0. We can see the null space graphed, which in a 2D setting where the vectors are linearly dependent, we can see that the null space is a line of vectors that is perpendicular to the linearly dependent vector, intersecting at the origin. In 3D space with one pair of linearly dependent vectors, the null space is also a line, intersecting at the origin at a 90 degree angle. However, in the case that all 3 basis vectors are linearly dependent, then set of vectors that are a part of null space is now a plane, since in a 3D space, there is a whole wide range of lines that can be perpendicular to another line, which combining the whole range results in a plane.

## Chapter 8 - Nonsquare Matrices as Transformations Between Dimensions
Understanding nonsquare matrices, the number of columns can represent the dimensions of our matrix, while the row represents the dimension that we are mapping on to. Visually, a 3x2 matrix would potentially map a 2D plane on to a 3D graph, and a 2x3 matrix would potentially map a 3D plane on to a 2D grid.

## Chapter 9/10/11 - Dot Product, Duality, Cross Product
Within this chapter, we explore more in depth behind what it geometrically means to solve for the volume of a parallelepiped. An understanding of the cross product, dot product, and duality is needed.

The overly simplified concept of duality is that when shifting from a higher level dimension to a lower one, for instance from 3D to 1D, we can use a 1x3 matrix to represent this change. This matrix can be seen as the transformation matrix that you can use to take a vector in 3D space, and place it on to the number line in 1D space. An important concept to understand about the duality of a vector is that it can easily be used as a vector in matrix vector multiplication, or as a factor in a dot product.

The dot product on a geometric level generates a projection of a vector on the appropriate respective vector, of which the product of the dot product is a scalar that can be used to find the appropriate length of the vector that is projected. Without the scalar, the project vector is the unit vector. The dot product can also be used to determine the if the relationship between two vectors is orthogonal or not. Note that dot product is commutative.
- (u dot v): v is projected on to u

The cross product is synonymous to the determinant, to which the value it obtains is the area between the two vectors given to it, assuming that the vectors create a parallelogram. As though the result of a cross product is a number, it does imply a vector, to which it is orthogonal to the two original vectors. To determine which direction the new orthogonal vector will go, you must use the right hand rule, such that in the example of (u cross v), u will be the index finger, v will be in middle finger, and the new vector x will be your thumb, no exceptions. The cross product can be negative yes, however in that event, it would mean that say in the example of (u cross v), v is on the right side of u. This would require you to rotate your right hand to the point where you're doing a thumbs down.
- (u cross v): u is the index finger, v is the middle finger, result is thumb

Getting to the Essence of Linear Algebra, I believe that a potential point that which the video may be trying to convey is that as long as no rules are broken: transformations are to leave the axes evenly spaced, rules that apply to one dimension, applies to all dimensions. From a 2D perspective, what does it mean to take the cross product? It means to compute an orthogonal vector to which its resulting length from multiplying the scalar would be the area of the product. What about the dot product? The dot product in correlation to duality helps us come to an understanding from a lower dimension of the resulting space that it occupies in a higher dimension. The dot product involving a 2D space with two 1D vectors results in a 1D vector, yet this vector tells us the area generated by the 2D vectors. At a 3D level, The dot product involving a 3D space with a 2D plane and a 1D vector generates a vector in the 1st dimension, to which its length represents the next step up from a plane, the volume of the object.

Rewatching the same lecture for hours, something finally clicked when he mentioned using a component of a vector perpendicular to your space occupying object. Just to reiterate what I mentioned in the last paragraph, I feel like I got to the correct answer from the wrong way, yet perhaps I'm not that far off of it to call it the wrong way.

Looking at 4D from curiosity, yea I don't know its not that straight forward and applicable to all dimensions or something. STICK TO COMPUTER GRAPHICS.

Some further notes, to find the volume of a parallelepiped, we must use the triple scalar product, which in the case of vectors u v and w, in no particular order, we must computer (u dot (v cross w)). We are to use the cross product of two vectors to find the area of the created 2D plane between them. We must then use the dot product to "reduce the dimensionality" to get the volume. Originally I thought that we should use the cross product of the original cross product (u cross (v cross w)), since its like the idea of length times width times height. Besides, we use the cross product of 1D vectors to generate the space in which it potentially occupies, in the plane above. Logically in my head, use the cross product again to determine the space it'll occupy in the 3rd dimension. The problem remains however, this is not the case. Thinking about it, the result of a cross product realistically is just another vector that's the length of the area defined. In the case of doing it again for the 3D volume, since we're doing it between TWO VECTORS AGAIN, you're realistically still in the same situation as before, just that now you're STILL finding the area of the plane between the new vector and the old third vector, and that is not the volume. To find the volume of a parallelepiped correctly, we must find the altitude of the object, or the height relative to the base parallelogram we found. Assuming that the final vector isn't always exactly perpendicular to create a rectangular prism, we must use the vector we found using the cross product, as that vector IS perpendicular to the base 2D plane, and actually project the old 3rd vector on to the perpendicular vector and use its Cosine value (adjacent hypotenuse since the perpendicular vector is the longest leg for sure, and the angle we're using is between the long leg and adjacent leg), which will generate a value between 1 and -1, MEANING BASED ON HOW ANGLED THE OLD 3RD VECTOR IS, IT WILL BE SCALED DOWN APPROPRIATELY TO MATCH THE PERPENDICULAR VECTOR IN TERMS OF ANGLE, AND BE THE CORRECT HEIGHT FOR THE ALTITUDE OF THE PARALLELEPIPED THAT WE'RE LOOKING FOR. tl;dr, just use the dot product bc its right.

## Chapter 12 - Cramer's Rule
#### JUST ME RAMBLING
Just as a definition to start, orthonormal transformations are one in which the basis vector remain perpendicular and at unit length. Before I ramble about Cramer's rule, I feel like the only reason this works is because we are forming a parallelogram in a 2D space. Because opposing sides are equal, and that determinants are scaled equivocally, we are able to trace backwards the transformation through computing how the area had morphed. Knowing how one axis morphed and how the opposing axis resulted allows us to go retrace the transformation. Oh I see, the issue I was having was that I was using the original basis vector without the transformation, we actually use the transformation matrix, and the resulting vector to create a parallelogram that we use to trace back. Since we are using the remnants of the transformation, there can only be one inverse of the transformation that leads us back to the original input. We know that the determinant is proportionally affected in a transformation, sidetrack: is it wrong to say that a parallelogram can realistically only be transformed in one iteration with no other possibilities? I mean that is what it means to be a function. A way that I'm think about this, is that if we fed this function a 1D line of vectors in the span space, we would see a smooth transition of the transformation. We are essentially scrolling through the transformation until we reach the desired output. Nah that seems really wrong. I think what I said isn't completely wrong, but its definitely not completely correct.

#### JUST ME RAMBLING
What does it mean necessarily to divide by a determinant geometrically? Apparently it means to "unstretch" the transformation. Given that we know the transformation, we should also know by how much the value has stretched, and if we know how much the value has stretched, we can go back in reverse to find the input vector before it has stretched. There's something that keeps coming back to my mind, that there is only one possible volume per value, which clearly isn't true from that statement alone, but perhaps theres something to ponder about? I'm sure I could relate this back to how we should use the transformed determinant of the resulting vector and the respective basis vectors. What guarantees that we are going back to the original vector? It's the fact that we are pinning the resulting vector TO THE TRANSFORMED BASIS VECTOR. We know for sure what the original basis vectors, are and by using a parallelogram that pegged to these basis vectors, we can trace back to the original input vector. We also do have to solve one by one for each axis. Bringing this back all the way to how a vector works, we essentially can break down all vectors to x(i hat) and y(k hat), where x and y are scalars and the hats respectively are the basis vectors. In computing the Area / det(A) where A is the transformation, we are one by one solving for each of the scalars of x and y here in Cramer's rule. By following the parallelogram back, we are able to find the appropriate scalar such that we are able to use the original basis vector to see where one of the components of the vector lands. in using "i hat" for one of the side of the parallelogram, we are able to solve for the opposite scalar of y. By pegging the resulting vector to one of the transformed basis vector, we are able to "take to back" to its original state pre-transformation by dividing by the determinant of the transformation. I feel like in a way, in the equation of Avec2(x,y) = vec2(x1,y1), where A is the transformation, by using this method, we avoided using the inverse of a matrix, which I guess this was the whole point, another method to solving for the system of equations.

#### Geometric Understanding and Summarization
In a more coherent thought, I believe that geometrically Cramer's rule takes advantage of the transformed basis vectors by pegging the resulting vector to one of them to determine the other. This is possible due to the back that the transformation can only warp a shape in that particular way, THERES ONLY ONE POSSIBLE WAY FORWARD, AS SUCH YOU TAKE THAT SAME WAY BACK. In doing so, we are able to get the opposing axis' scalar. This is in the context of 2D. In a 3D sense, I believe that we are doing the same computation, except that we are pegging the resulting vector to two out of three of the basis vectors, and repeat it 3 times.
