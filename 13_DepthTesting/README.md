# Contents of this Branch
This module brings to light how OpenGL determines which pixels should be in front. This seems to be more of a module that brings awareness to the functionality behind camera frustums. FragCoords represent the screen that we see, with its x and y values representing the coordinates mapped on to our window linearly. These values are calculated through the MVP matrix being applied to the objects and camera frustum respectively. Along with that, we also receive the z proponent through the same calculations, however it is important to note that unlike the other proponents, z is not linear. This is due to the way that the objects are warped to fit on to screen space, effectively squishing the object onto the tip of a pyramid. As z is on the axis that gets warped in the procedure, the z values are not linear.

When it comes to moving from view space to clip space, it is important to note about precision. Imagining a pyramid starting from the base, as it goes up to the tip, the square base gets compressed. The more compressed the square gets, the less precise our view will be. This can cause some precision errors like z-fighting, where if two faces are layered and occupying the same space, then OpenGL will not be able to tell which face should be shown, and will flicker between the two. If there is a bit of space between the layers, then then can avoid some z-fighting. However if we lose enough precision, then despite there being a bit of space, z-fighting may happen. As such, a common method to avoid z-fighting is to increase the precision by moving the near plane further away from the camera and closer to the far plane of the frustum.

local space -> world space -> view space -> clip space -> normal device coordinates -> screen space

https://www.youtube.com/watch?v=U0_ONQQ5ZNM <- best video I've seen for this topic

SKIP THIS PARAGRAPH, ITS REALLY JUST ME RAMBLING UNLESS YOU WANT TO READ IT BUT ITS CONFUSING
There's a lot of math that goes under the hood and is thus not brought to attention when more respect should be placed on it. For the basics of OpenGL, we have the concept of the MVP matrix, which in the "space pipeline" listed above, each of the specific matrix of MVP brings us closer to the end goal of screen space. The model matrix takes us from local space to world space, view matrix takes us then to view space, and projection lastly takes us to clip space. From here, we are still not too close to screen space, which begs the question of how OpenGL gets us the rest of the way there. This is where Normalized Device Coordinates come in to play to get calculated and outputted on to screen space. Under the hood, there are two more matrices that we should be wary of; the orthographic projection matrix, and the viewport transformation matrix. Starting with the orthographic projection matrix, this matrix grants us the NDC that correlates to the canonical view volume, which is simply a cube or a rectangular prism depending on the graphics language you're using, which is the volume in which will be seen by the camera, essentially be the area thats not culled by the camera automatically. Our view volume is not always stationary, and the camera we use is dynamic, as such we need to be able to calculate the coordinate in a way such that it gets transformed back to where the canonical view volume is for us to see. The matrix is made up to two matrix transformations: a translation, and a scale. The canonical view volume is always at the origin, while our camera isn't always. As such, we are to translate our view volume to the center, and then scale it properly to be 1:1 with the canonical view volume. THE VOLUME HERE IS WHERE NORMALIZED DEVICE COORDINATES RESIDE. This is what we're finding, and this is what we see through our screen, if we had an orthographic camera and skipped converting to a frustum view. Regardless of our camera type, we must still multiply by viewport transformation matrix to get to screen space. This matrix is what gives us the depth effect, with objects far away being smaller. The video will do a better job of explaining this part, but I'll do what I can here. The viewport transformation matrix relies on triangle proportionality, where two triangles of different sizes yet the same angles will have side lengths that are proportional. This is how the x and y component of the matrix is solved, essentially scaling down the far size of the frustum to for screen space. This equation actually requires us to divide by the z proponent, which in a 3x3 matrix isn't possible to do, which is why we need the 4th component to properly express this, using the w proponent. By setting the W proponent to Z, we can divide the coordinates by the value Z. The problem now is to determine how to properly scale down the z proponent to the smaller triangle. The video doesn't exactly state why, but we want to keep the z value to remain as z AFTER dividing by w, which means the product must result in z^2 so that z^2/z = z. Just as a rationalization, perhaps the reason as to why we dont scale down the z value is because we are still basing our view on the actual z value, we're just doing our best to simulate depth by using the square frustum. To simulate the "closeness" of an object, we kind of shrink the resolution near the camera. The problem I was having was that I was assuming was that the near plane is our screen, but I feel like thats wrong to say. We are able to see the entire contents of the frustum, its not like everything gets projected on to the near plane, I don't think I should be thinking that 2-Dimensionally. The screen itself is 2D, and the image thats rendered on to our screen is also 2D, but doesn't mean the space is 2D, I think thats the problem I'm having. The viewpoint matrix is what shapes our frustum to look like that truncated pyramid, thats probably all there is to it, don't complicate it more. It's not like we are trying to squeeze everything inside the frustum on to the near plane, thats where it went wrong in my head. I assumed that we're trying to map the contents of the frustum on to the near plane, and that's what we see, but that's not what the math does. The math defines the space in which we're allowed to see in. As such, its not that we're scaling down the x and y proponent from the far plane down to the near plane, we're literally just calculating if the fragment is inside the frustum. Thats why we want z to remain as z. Fragments naturally have a z component already, which I presume is calculated for us to find the depth when we find the normalized device cordinates moving to the canonical view volume.

BOTTOM LINE:
clip space -> canonical view space
The orthographic projection matrix is used to transform our current view space to the canonical view space, which is the space in which our normalized device coordinates occupy (a cube or rectagular prism), getting our x and y values for the position in space, and the z value for its depth (the coordinates of gl_FragCoord once normalized to the range of 0 to 1).
This should give enough of an idea of how it all works once you realize what the canonical view space is, which is just a volume at the origin that varies based on the graphics language being used. This volume is essentially what we seen, and we need to transform what we see to this canonical volume.

canonical view space -> screen space
The perspective matrix or the viewport transformation matrix is what converts our coordinates fully to screen space, essentially being the matrix that defines our frustum, allowing us to see as far as the z value is, in the varying frustum range of x and y.


By the way, this does explain as to why we the z value is nonlinear. It is clear as to why we want the z value to remain as z, because its the distance that we're supposed to be able to see, but to stay as z after the division from w value, z must equate to z^2, which the video shows the math for that. Since it becomes a quadratic equation, its not linear, so we do our best to make it linear.

There are many questions to this, but I don't think I'll be able to solve it, rather perhaps its not worth solving on my own. I feel like I already have too many assumptions that I think are right, but going off of too many assumptions will lead nothing good probably. I believe my current understanding is sufficient, and already way better than before. I feel confident now in moving forward.

While research I came across something really interesting. So to restate, the canonical view in OpenGL is a cube, ranging from -1 to 1 in all axes. This entire volume where the NDCs reside, and its also essentially the area that gets morphed to fit the frustum. I think, these despite these values ranging from -1 to 1, these values are the FragCoords used in the shader, its just so happens that they get normalized to the range 0 to 1 insteadof -1 to 1. To do so, you add 1 to all values, thus making the range become 0 to 2, then you divide by 2, to make it 0 to 1. This also explains how in some other posts, and as well as this module, saying that you can use the actual linear depth value by initally multiply by 2, then subtract by 1. I think theres more math after that to get the actual linear z value, but I think this does confirm my logic.
