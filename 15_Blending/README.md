# Contents of this Branch
I wanted to get something as a contribution for the day, however I more or less contemplated my goals for the day, something feels off continuing to learning OpenGL at this point, rather I think its probably best to start nitpicking things to learn. I can't say for sure what exactly I want to do, but I feel committed to graphics programming at this point. I very much do feel much more confident in learning in dependently after conducting extensive research to really get an understanding of the previous topics, like depth, spot light effects, and camera movements. I think starting tomorrow, I'll take a break from LearnOpenGL.com, and start on a real project, and conduct research on my own if I need it. Of course learning these things fundamentals(?) are important as I'm sure that I'll use them eventually. How would I know what to look up if I don't know it exists in the first place. In all honesty, I'm definitely learning the wrong way, I need to make some projects on the side or else just learning the skill and doing nothing with it, I'm just going to forget it, but also I'm going to lose interest if I'm not stimulating my creativity.

I'm Back:
It's difficult to justify learning certain topics when there's a module that covers that topic, and all it takes is simply having the patience to learn it. I'm going to continue learning through the modules, and push through it with the goal of completing my personal project.

There was actually a really interesting topic when it comes to rendering transparent objects like windows, where the order in which you render the objects do matter, as sometimes a window will not be visible behind another window because the window in front got rendered first. Due to depth buffer optimizations, if an object is supposed to exist behind another object, but the direct view has been blocked, the object will get culled to save processing power. Since we are working with transparent values, we can visibly see this in action. There is a way to avoid this, in which we must forcefully render the object from furthest to closest, so that the depth buffer check doesn't ignore rendering the object. This required some form of sorting of rendering the objects, which begs the question, in a game made up of entirely glass and transparent objects, how taxing is it to realistically sort all the glass such that we render them correctly? Apparently, this method is comment in many games, so I guess it its accepted as a computation thats necessary. According to ChatGPT, most high-end games use depth peeling, or weighted blended order-independent transparency, both of which are order-independent. Depth peeling is essentially checking each "layer" of the frustum starting from the end, and rendering the glass in that manner until it reaches the camera. WBOIT, is the interesting one and the one that I'm assuming is the industry standard for good performance. So, this method of transparent rendering does not solve the overlapping glass panes issue, however it cuts down on the performance cost by estimating the color of a pixel through multiple layers of transparent object. WBOIT runs through and computes together the color and transparency of a pixel based on whats visible. It takes the weighted average of the color and alpha value, and outputs it. There is a series of equations that it still needs to go through, however after reading an article, the gist of WBOIT seems like it acts as a filter, and for each pixel, count the number of objects layered, run the calculations to find the weighted average of colors and alpha values, then display the output. This is likely to be used in conjunction with the DepthMask disable while rendering the transparent objects, which leads to some weird optical illusions, but they get blended to the point where they could be believable. WBOIT as a benefit does not need to sort, and can be done in one pass through, thus being very efficient, but its not perfect. There are a lot of issues in the equations that lead to some undesirable outcome since its trying to find the average transparency, and not the individual transparency. A good limitation to put to ensure some quality is to not make the values to opaque. Just off of some observation from looking at the equations, it feels like keeping the transparency of the objects to be the same alpha values may also lead to better results, but that could be a wrong assumption. Overall, it was definitely worth looking into, and interesting to see the trade offs between quality and performance.

Article Read: https://habr.com/en/articles/457292/
